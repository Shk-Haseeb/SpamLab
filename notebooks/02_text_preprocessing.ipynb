{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c166bf58-2ddf-49f7-8205-4ed55125a047",
   "metadata": {},
   "source": [
    "#  Notebook 02: Text Preprocessing for Spam Classification\n",
    "\n",
    "### ðŸ§¾ Overview\n",
    "In this notebook, Iâ€™ll clean and prepare the email text so it's ready for machine learning.  \n",
    "The goal is to go from raw email bodies to simplified, tokenized, and filtered text that captures the most meaningful words.\n",
    "\n",
    "---\n",
    "\n",
    "## Loading Libraries and Dataset\n",
    "\n",
    "I start by:\n",
    "- Loading the same `load_spam_ham_data()` function from the previous notebook\n",
    "- Re-importing the cleaned raw text dataset\n",
    "- Importing NLTK (Natural Language Toolkit) to handle tokenization and stopword removal\n",
    "\n",
    "---\n",
    "\n",
    "### Why `nltk.download()` is used:\n",
    "NLTK doesnâ€™t come with all data by default. I need to download:\n",
    "- `'punkt'`: for tokenizing sentences/words\n",
    "- `'stopwords'`: a list of common English words to filter out (like \"the\", \"and\", \"is\")\n",
    "\n",
    "These downloads only need to happen once and will be skipped if already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6f82b5-a4f2-4ccc-be95-c42a608baadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data import load_spam_ham_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fc27e3-a63a-47cf-9d45-fc03772ebaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_spam_ham_data(base_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5435d0-c9a2-4e65-9f90-69d216ebb155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shhaseeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shhaseeb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180d32b-5b37-421f-9ff6-702df9c70cfa",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Text Preprocessing\n",
    "\n",
    "To prepare the email text for modeling, I define a simple preprocessing function that:\n",
    "\n",
    "- Removes punctuation and symbols using regex\n",
    "- Converts text to lowercase\n",
    "- Tokenizes into words\n",
    "- Removes stopwords (like \"the\", \"and\", etc.)\n",
    "- Filters out very short words\n",
    "\n",
    "---\n",
    "\n",
    "I also use `strip_email_headers()` to remove metadata at the top of each email.\n",
    "\n",
    "The final cleaned text is saved in a new column: `processed_text`.  \n",
    "This version is now ready for TF-IDF vectorization in the next notebook.\n",
    "\n",
    "Finally, I export the cleaned data as `processed_emails.csv` to reuse during modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0566b285-b006-4994-b901-814ba5ce6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    filtered_words = [w for w in words if w not in stop_words and len(w) > 1]\n",
    "\n",
    "    return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b7241b-3412-47d8-93bf-6301f18f2466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_email_headers(text):\n",
    "    return text.split('\\n\\n', 1)[-1]\n",
    "\n",
    "df['clean_text'] = df['text'].apply(strip_email_headers).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd24d49d-6095-491f-8891-dde869fbc7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>nextpart content type text html charset iso co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>mailings sent complying proposed unsolicited c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>need health insurance addition featuring large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>html align center font ptsize family sansserif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>worldwide great restaurants shopping activitie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                     processed_text\n",
       "0  spam  nextpart content type text html charset iso co...\n",
       "1  spam  mailings sent complying proposed unsolicited c...\n",
       "2  spam  need health insurance addition featuring large...\n",
       "3  spam  html align center font ptsize family sansserif...\n",
       "4  spam  worldwide great restaurants shopping activitie..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text'] = df['clean_text'].apply(preprocess_text)\n",
    "df[['label', 'processed_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5333cfcd-632f-4afd-82f6-adb1f58c6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed_emails.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spamlab)",
   "language": "python",
   "name": "spamlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
